{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ea0bd-7823-47cb-a4cf-2dc6c878c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librerias y peticion al servidor\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6add5-de21-40b6-9fbc-00de7654ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_inicial='http://www.rmiq.org/ojs311/index.php/rmiq/issue/archive'\n",
    "url_root= 'http://www.rmiq.org/ojs311/index.php/rmiq/issue/archive'\n",
    "r = requests.get(url_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bda294-2123-4f7d-8ca5-caf817c57740",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7607c9-d42c-4939-9b38-ac6951a1acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#primera funcion para obtener links de los volumenes \n",
    "box = soup.find('ul', class_='issues_archive')\n",
    "volumen=box.findAll('div',class_='obj_issue_summary')\n",
    "vol = [x.find('a').get('href')for x in volumen]\n",
    "vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241e92a-ad37-481f-b9a0-72ff513f9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui se realiza una lista donde la variable vol contendra los links de los articulos dentro de cada volumen,\n",
    "#con estos links obtenidos, se implemeta un ciclo para obtener todos los links de los articulos\n",
    "vol2 =[]\n",
    "for i in vol: \n",
    "    url_inicial1=i \n",
    "    r1 = requests.get(url_inicial1)\n",
    "    soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    box1 = soup1.find('ul', class_='cmp_article_list articles') \n",
    "    volumen1=box1.findAll('div', class_='obj_article_summary')\n",
    "    vol1 = [x.find('a').get('href')for x in volumen1]\n",
    "    vol2+=vol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09accf-9c56-4054-b0ff-6dd1e8f72b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprime los articulos\n",
    "vol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e10f95-3273-4454-a5fd-8d1703995ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lee y guarda cada uno de los links obtenidos\n",
    "def get_url_items(sopa,url):\n",
    "    box1 = soup1.find('ul', class_='cmp_article_list articles') \n",
    "    volumen1=box1.findAll('div', class_='obj_article_summary')\n",
    "    vol1 = [x.find('a').get('href')for x in volumen1]\n",
    "    return vol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42598dbf-d38b-4713-81bc-8f1de772c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para encontrar la paginacion e ir iterando en cada una y obtener los links a scrapear en ese momento\n",
    "links_items=[]\n",
    "i=0\n",
    "while i<1:\n",
    "    i+=1\n",
    "    print(f'Estoy en la pagina {url_inicial}')\n",
    "    r_pag=requests.get(url_inicial)\n",
    "    s_p=BeautifulSoup(r_pag.text,'html.parser')\n",
    "    links=get_url_items(s_p, url_inicial)\n",
    "    links_items.append(links) \n",
    "    next_a=s_p.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487395e-2ea4-48fa-ab11-1dfe05306943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#esta funcion lo que hace es un listado de los articulos que se encontraron y dar el numero exacto\n",
    "list_scraper=[]\n",
    "for i in links_items:\n",
    "    for j in i:\n",
    "        list_scraper.append(j)\n",
    "len(list_scraper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be0404-c125-4760-bc39-e320d211a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se iran tomado uno a uno los links para scrapearlos\n",
    "uno=list_scraper[0]\n",
    "r_item=requests.get(uno)\n",
    "s_item=BeautifulSoup(r_item.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9311a1c-607b-47ba-8234-ed71c858bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para iniciar el escraper de cada libro\n",
    "def scraper_book(url):\n",
    "    content_book={}#diccionario que no va acumulando la imformacion para el tada frame\n",
    "    r=requests.get(url)\n",
    "    tire='Revista Mexicana de Ingeniería Química'\n",
    "    a='VII. Ingenierías'\n",
    "    tem='Ingeniería Química y sus interfaces con otras disciplinas de la Ingeniería: Termodinámica Catálisis, Cinética y Reactores; Simulación y Control; Fenómenos de Transporte; Seguridad; Ingeniería de Procesos; Biotecnología; Ingeniería de Alimentos'\n",
    "    s_item=BeautifulSoup(r.text,'html.parser')\n",
    "    #titulo de revista\n",
    "    try:\n",
    "        titulo=tire\n",
    "        content_book['Titulo Revista']=tire\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Revista']=None\n",
    "    #area\n",
    "    try:\n",
    "        area=a\n",
    "        content_book['Area']=a\n",
    "    except AttributeError:\n",
    "        content_book['Area']=None\n",
    "    #tematica\n",
    "    try:\n",
    "        tema=tem\n",
    "        content_book['Tematica']=tem\n",
    "    except AttributeError:\n",
    "        content_book['Tematica']=None\n",
    "    #titulo articulo\n",
    "    try:\n",
    "        titu=s_item.find('h1', class_='page_title').get_text(strip=True)\n",
    "        content_book['Titulo Articulo']=titu\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Articulo']=None\n",
    "    #resumen\n",
    "    try:\n",
    "        resu=s_item.find('div', class_='item cover_image').get_text(strip=True)\n",
    "        content_book['Resumen']=resu\n",
    "    except AttributeError:\n",
    "        content_book['Resumen']=None\n",
    "    #abstract\n",
    "    try:\n",
    "        abst=s_item.find('p').get_text(strip=True)\n",
    "        content_book['Abstract']=abst\n",
    "    except AttributeError:\n",
    "        content_book['Abstract']=None\n",
    "    #Link incial\n",
    "    try:\n",
    "        linkart=url\n",
    "        content_book['Link Articulo']=linkart\n",
    "    except AttributeError:\n",
    "        content_book['Link Articulo']=None\n",
    "    #link articulo\n",
    "    try:\n",
    "        link=s_item.find('a', class_='obj_galley_link pdf').get('href')\n",
    "        content_book['Link PDF']=urljoin(url_root, link)\n",
    "    except AttributeError:\n",
    "        content_book['Link PDF']=None\n",
    "    return content_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10960c8-240a-4b36-a144-b02c0cd6d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#esta funcion comienza a scrapear cada uno de los articulos para obtener la informacion requerida\n",
    "list_scraper=list_scraper[0:469]\n",
    "datos_book=[]\n",
    "for idx, i in enumerate(list_scraper):\n",
    "    print(f'estas escrapeando la pag {idx}')\n",
    "    datos_book.append(scraper_book(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225fa55-b0bd-46ba-851b-8175feebfd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#una vez terminado el scraping se imprimen los datos en un dataframe\n",
    "df_catalogo=pd.DataFrame(datos_book)\n",
    "df_catalogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13d6b9-9965-4eb0-a8a4-5e87641e18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si se detecta alguna infotmacion repetida o que no es importante con esta funcion se elimina la fila\n",
    "df=df_catalogo.drop(df_catalogo.index[[47, 303]])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c764f10-296a-42c8-94ff-062b772e5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ya teniendo la tabla con la informacion deseada se pasa a guardar como un archivo CSV\n",
    "df.to_csv('Revista005.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5b176-19a3-45e8-a809-d2a0b7c3f7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
