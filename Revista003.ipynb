{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ab668b-a26c-440c-90b3-0848a6c7bcb5",
   "metadata": {},
   "source": [
    "# WEB SCRAPING\n",
    "  \n",
    "Se refiere al proceso de extracción de contenidos y datos de sitios web mediante software con el fin de analizarlos o manipularlos en otros medios, para la cual se utilizan bots para extraer los datos y contenidos de las webs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32409d8-f7d0-443a-a798-d9bbd580f9cc",
   "metadata": {},
   "source": [
    "Importación de librerías.\n",
    "\n",
    "Requests realiza la petición al servidor.\n",
    "BeautifulSoup analizar documentos HTML.\n",
    "Pandas podemos representar datos tabulares con columnas con etiquetas y filas y series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b137d1-a97c-40b6-bcfc-fba9127df2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924b965-ce98-4ec0-9d20-0fe79ee698a9",
   "metadata": {},
   "source": [
    "Se realiza un request de la url_inicial de la revista, url_root nos ayuda a genera un url completa a partir de la url raíz para esto se utiliza la librería urljoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a72335-3f31-42a2-9fe0-6a0a62da6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_inicial = 'http://www.revistas.unam.mx/index.php/geofisica/issue/archive?issuesPage=6#issues'\n",
    "url_root = 'http://www.revistas.unam.mx/index.php/geofisica/issue/archive?issuesPage=6#issues'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71664e30-5442-485d-a080-0824077de940",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=requests.get(url_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b2593-bac0-4b8c-bf60-7ec9af2a2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90138d2-554c-4b0b-88ad-4752cad4d8ac",
   "metadata": {},
   "source": [
    "Obtención de los primeros volúmenes.\n",
    "\n",
    "Se realiza la búsqueda para obtener los urls de cada uno de los volúmenes o archivos de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f41789-b38d-49eb-ae99-9fdb3f757f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = soup.find('div', id='issues')\n",
    "volumen=box.findAll('h4')\n",
    "vol = [x.find('a').get('href')for x in volumen]\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8f5660-dece-4299-b73a-cca750bcbf26",
   "metadata": {},
   "source": [
    "Obtención de los segundos urls.\n",
    "\n",
    "Se realiza una lista en la que la variable *vol* que contiene las urls de cada uno de los volúmenes publicados de la revista, con estos urls obtenidos,  con la variable *vol* se implementa un ciclo en el cual se utiliza la variable *vol2* de la cual se van acumulando las siguientes urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ff644-7ace-4911-acf9-739d540a1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2 =[]\n",
    "for i in vol: \n",
    "    url_inicial1=i \n",
    "    r1 = requests.get(url_inicial1)\n",
    "    soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    box1 = soup1.find('div', id='content') \n",
    "    volumen1=box1.findAll('ul', class_='menu')\n",
    "    vol1 = [x.find('a').get('href')for x in volumen1]\n",
    "    vol2+=vol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e78f69-faed-4d28-b67e-08657007d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0804de-5c06-4290-bc44-c142d66b22df",
   "metadata": {},
   "source": [
    "Obtención de las ulrs de los artículos.\n",
    "\n",
    "Algunas revistas necesitan más búsquedas para llega a los artículos ya que algunos manejan primero los volúmenes o archivos y de eso archivos se dirigen a un urls que es la tabla contenidos, donde se encuentran los artículos, por lo cual se comienza a realizar la misma búsqueda que en *vol2*.\n",
    "\n",
    "Se implementa otro ciclo con *vol2*, para esto se utiliza la variable de *vol3* que recolecta las urls de los artículos, para comenzar con su scrapeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da257aa-7dac-485c-895f-68560f4e86a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol3 =[]\n",
    "for b in vol2: \n",
    "    url_inicial2=b \n",
    "    r2 = requests.get(url_inicial2)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "    box2 = soup2.find('div', id='content') \n",
    "    volumen2=box2.findAll('a')\n",
    "    vol2= [o.get('href')for o in volumen2]\n",
    "    vol3+=vol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df220e8-5577-4770-85e8-3eb644017a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31483e-cd07-41ba-b7b7-d65e1a57eba4",
   "metadata": {},
   "source": [
    "En esta función variable sopa se utiliza para tener las paginaciones de la revista y la variable url es guardan cada uno de url que ya se obtuvieron de *vol3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770163b-9688-4bd2-bbc2-2d5a8a6e99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_items(sopa,url):\n",
    "    box2 = soup2.find('div', id='content') \n",
    "    volumen2=box2.findAll('a')\n",
    "    vol2= [o.get('href')for o in volumen2]\n",
    "    return vol3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bdaf97-f8a7-4d54-8ae0-5ea022e9d30f",
   "metadata": {},
   "source": [
    "Ahora se va acumulando cada uno de los links e ir iterando en cada una de las páginas, para traer cada uno de los links que se van a ir scrapeando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78ec7b-1874-4207-b6e8-a55e91ddbe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_items=[]\n",
    "i=0\n",
    "while i<1:\n",
    "    i+=1\n",
    "    print(f'Estoy en la pagina {url_inicial}')\n",
    "    r_pag=requests.get(url_inicial)\n",
    "    s_p=BeautifulSoup(r_pag.text,'html.parser')\n",
    "    links=get_url_items(s_p, url_inicial)\n",
    "    links_items.append(links) \n",
    "    next_a=s_p.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedc097-fea2-49b5-82ab-6cc10bfc8074",
   "metadata": {},
   "source": [
    "Obtiene el número de los artículos que se encontraron en los primeros volúmenes de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f8416-7445-4f71-b703-61289d9cdde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=[]\n",
    "for i in links_items:\n",
    "    for j in i:\n",
    "        list_scraper.append(j)\n",
    "len(list_scraper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db3f5e-fb40-4a76-901c-3d29f97a2a74",
   "metadata": {},
   "source": [
    "Toma uno a uno de los links donde se encuentra la información de cada artículo para Scrapearlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fefd96-e8bd-42f6-b456-9ad6f5dfd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uno=list_scraper[0]\n",
    "r_item=requests.get(uno)\n",
    "s_item=BeautifulSoup(r_item.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cb408c-c0c1-4df4-a3e5-76f96f9e9c76",
   "metadata": {},
   "source": [
    "Scrapeo de las urls de los ariculos.\n",
    "\n",
    "Mediante esta función revisa cada uno de los links de los artículos en los cuales se encuentra la información\n",
    "del artículo, por lo cual se aplican cada uno de los métodos, que nos van a obtener lo que estamos requiriendo de dicho artículo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6701cdc-c2fb-45b6-b21b-58bb3a7f04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_book(url):\n",
    "    content_book={}\n",
    "    r=requests.get(url)\n",
    "    tire='Geofísica Internacional'\n",
    "    a='I. Físico Matemáticas y Ciencias de la Tierra'\n",
    "    tem='Sismología, Vulcanología, Ciencias Espaciales, Hidrología y Exploración, Paleomagnetismo y Tectónicas, Oceanografía Física'\n",
    "    s_item=BeautifulSoup(r.text,'html.parser')\n",
    "    #titulo de revista\n",
    "    try:\n",
    "        titulo=tire\n",
    "        content_book['Titulo Revista']=tire\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Revista']=None\n",
    "    #area\n",
    "    try:\n",
    "        area=a\n",
    "        content_book['Area']=a\n",
    "    except AttributeError:\n",
    "        content_book['Area']=None\n",
    "    #tematica\n",
    "    try:\n",
    "        tema=tem\n",
    "        content_book['Tematica']=tem\n",
    "    except AttributeError:\n",
    "        content_book['Tematica']=None\n",
    "    #titulo articulo\n",
    "    try:\n",
    "        titu=s_item.find('div', id='articleTitle').get_text(strip=True)\n",
    "        content_book['Titulo Articulo']=titu\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Articulo']=None\n",
    "    #resumen\n",
    "    try:\n",
    "        resu=s_item.find('div', id='articleAbstract').get_text(strip=True)\n",
    "        content_book['Resumen']=resu.replace('Resumen','')\n",
    "    except AttributeError:\n",
    "        content_book['Resumen']=None\n",
    "    #abstract\n",
    "    try:\n",
    "        abst=s_item.find('div', class_='item cover_image').get_text(strip=True)\n",
    "        content_book['Abstract']=abst\n",
    "    except AttributeError:\n",
    "        content_book['Abstract']=None\n",
    "    #Link incial\n",
    "    try:\n",
    "        linkart=url\n",
    "        content_book['Link Articulo']=linkart\n",
    "    except AttributeError:\n",
    "        content_book['Link Articulo']=None\n",
    "    #link articulo\n",
    "    try:\n",
    "        link=s_item.find('a', class_='file').get('href')\n",
    "        content_book['Link PDF']=urljoin(url_root, link)\n",
    "    except AttributeError:\n",
    "        content_book['Link PDF']=None\n",
    "    return content_book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d98c7-8727-48e6-87d0-f0cfae1ee045",
   "metadata": {},
   "source": [
    "El list_scraper hace un scrapeo de la cada uno de los artículos, ya que en datos_book se acumuló la información obtenida de lo métodos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd124d-c075-42ea-97ae-1ba161e6a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=list_scraper[0:120]\n",
    "datos_book=[]\n",
    "for idx, i in enumerate(list_scraper):\n",
    "    print(f'estas escrapeando la pag {idx}')\n",
    "    datos_book.append(scraper_book(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f100815-a8d4-4e62-9e1c-32534a686d60",
   "metadata": {},
   "source": [
    "Se obtiene la información requerida de los artículos.\n",
    "\n",
    "La variable **datos_book** es un listado en el que se recolecto la información requerida por lo cual\n",
    "se convierte en un **DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcc528-f17e-4706-a6f8-06eef109f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogo=pd.DataFrame(datos_book)\n",
    "df_catalogo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a92599-dd05-421f-8920-0f8729ee1396",
   "metadata": {},
   "source": [
    "En el caso de esta revista solo de deseaba obtener los artículos, por lo cual\n",
    "en algunas ocasiones se obtienes cosas diferentes, para esto con la función isin lo que hace es \n",
    "eliminar mediante el **Links PDF** los archivos que no se desean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d5e29-7674-4b0f-add5-937d08f4e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_catalogo[~ df_catalogo[\"Link PDF\"].isin(['None'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c2260-e409-4b7c-8684-1cddf4d2ca2a",
   "metadata": {},
   "source": [
    "La información solicitada se pasa a un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b0098-46e2-4c71-a397-de140e113bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Revista003.06.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9718ea8-8da7-42e8-8dac-06c9a71e3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "La información solicitada se pasa a un CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796cf3a7-f55f-43dc-bbd8-a10928346052",
   "metadata": {},
   "source": [
    "Final del Código"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
