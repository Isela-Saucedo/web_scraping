{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6908c1c7-1862-40b1-b06e-9d8e9d927fe9",
   "metadata": {},
   "source": [
    "# WEB SCRAPING\n",
    "  \n",
    "Es una técnica para extraer y almacenar datos de una o varias páginas web con el fin de analizarlos o manipularlos en otros medios, para la cual se utilizan bots para extraer los datos y contenidos de las webs. \n",
    "\n",
    "### Importación de librerías.\n",
    "\n",
    "* Requests realiza la petición al servidor.\n",
    "\n",
    "* BeautifulSoup analizar documentos HTML.\n",
    "\n",
    "* Pandas podemos representar datos tabulares con columnas con etiquetas y filas y series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4326137-203c-4c8e-add7-60b122a5955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161f985-bba3-44a9-b10f-38aa1ee95d13",
   "metadata": {},
   "source": [
    "Se realiza un request de la url_inicial de la revista, url_root nos ayuda a genera un url completa a partir de la url raíz para esto se utiliza la librería urljoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42dbd1-ff66-42c5-8447-68987cfebc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_inicial = 'https://estudiosdeasiayafrica.colmex.mx/index.php/eaa/issue/archive'\n",
    "url_root = 'https://estudiosdeasiayafrica.colmex.mx/index.php/eaa/issue/archive'\n",
    "r=requests.get(url_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a9433-6cf4-4fe0-b7ab-fc8f9c95c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e6614-c2c4-430c-ba74-0d4a33668df2",
   "metadata": {},
   "source": [
    "Obtención de los primeros volúmenes.\n",
    "\n",
    "Se realiza la búsqueda para obtener los urls de cada uno de los volúmenes o archivos de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73d6cd-19b5-4184-a7e8-d4643b09b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = soup.find('div', class_='container page-archives')\n",
    "volumen=box.findAll('div', class_='col-md-3 col-lg-2')\n",
    "vol = [x.find('a').get('href')for x in volumen]\n",
    "vol=[urljoin(url_root,i) for i in vol]\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc8b1de-d81a-42eb-ae75-14a40b808524",
   "metadata": {},
   "source": [
    "Obtención de los segundos urls.\n",
    "\n",
    "Se realiza una lista en la que la variable vol que contiene las urls de cada uno de los volúmenes publicados de la revista, con estos urls obtenidos, con la variable vol se implementa un ciclo en el cual se utiliza la variable vol2 de la cual se van acumulando las siguientes urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf81463-72cf-4b4d-8a98-c750d6714653",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2 =[]\n",
    "for i in vol: \n",
    "    url_inicial1=i \n",
    "    r1 = requests.get(url_inicial1)\n",
    "    soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
    "    box1 = soup1.find('div', class_='issue-toc-section') \n",
    "    volumen1=box1.findAll('div', class_='article-summary')\n",
    "    vol1 = [x.find('a').get('href')for x in volumen1]\n",
    "    vol2+=vol1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc969b-d232-480c-90d3-fb2d4f658a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38814e59-c143-4f6c-b31b-bbe754c42c5d",
   "metadata": {},
   "source": [
    "En esta función variable sopa se utiliza para tener las paginaciones de la revista y la variable url es guardan cada uno de url que ya se obtuvieron de *vol2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc747d8a-4cdc-4097-bd19-3319d30b4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_items(sopa,url):\n",
    "    box1 = soup1.find('div', class_='issue-toc-section') \n",
    "    volumen1=box1.findAll('div', class_='article-summary')\n",
    "    vol1 = [x.find('a').get('href')for x in volumen1]\n",
    "    return vol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7f55e-f80d-4678-9be7-336d58052863",
   "metadata": {},
   "source": [
    "Ahora se va acumulando cada uno de los links e ir iterando en cada una de las páginas, para traer cada uno de los links que se van a ir scrapeando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816b38d-b35e-4607-be89-c6052601eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_items=[]\n",
    "i=0\n",
    "while i<1:\n",
    "    i+=1\n",
    "    print(f'Estoy en la pagina {url_inicial}')\n",
    "    r_pag=requests.get(url_inicial)\n",
    "    s_p=BeautifulSoup(r_pag.text,'html.parser')\n",
    "    links=get_url_items(s_p, url_inicial)\n",
    "    links_items.append(links) \n",
    "    next_a=s_p.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994eb70-8b06-4666-a3d8-3330316b74fb",
   "metadata": {},
   "source": [
    "Obtiene el número de los artículos que se encontraron en los primeros volúmenes de la revista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2773b5-de65-40e4-885e-aab5917be849",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=[]\n",
    "for i in links_items:\n",
    "    for j in i:\n",
    "        list_scraper.append(j)\n",
    "len(list_scraper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bdbe5f-c58f-43ee-ac20-7534223711b4",
   "metadata": {},
   "source": [
    "Toma uno a uno de los links donde se encuentra la información de cada artículo para Scrapearlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6408d-dced-408d-8e1c-16c8c8bc9fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uno=list_scraper[0]\n",
    "r_item=requests.get(uno)\n",
    "s_item=BeautifulSoup(r_item.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f5afa-f29c-47a6-8fe0-ffeae39e7581",
   "metadata": {},
   "source": [
    "Scrapeo de las urls de los ariculos.\n",
    "\n",
    "Mediante esta función revisa cada uno de los links de los artículos en los cuales se encuentra la información del artículo, por lo cual se aplican cada uno de los métodos, que nos van a obtener lo que estamos requiriendo de dicho artículo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfed8d8-555b-484d-a97e-41fb230737ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para iniciar el escraper de cada libro\n",
    "def scraper_book(url):\n",
    "    content_book={}\n",
    "    r=requests.get(url)\n",
    "    tire='Estudios de Asia y África'\n",
    "    a='V. Ciencias Sociales'\n",
    "    tem='Historia y ciencias sociales, económicas y políticas, así como estudios culturales de los pueblos de Asia y África'\n",
    "    s_item=BeautifulSoup(r.text,'html.parser')\n",
    "    #titulo de revista\n",
    "    try:\n",
    "        titulo=tire\n",
    "        content_book['Titulo Revista']=tire\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Revista']=None\n",
    "    #area\n",
    "    try:\n",
    "        area=a\n",
    "        content_book['Area']=a\n",
    "    except AttributeError:\n",
    "        content_book['Area']=None\n",
    "    #tematica\n",
    "    try:\n",
    "        tema=tem\n",
    "        content_book['Tematica']=tem\n",
    "    except AttributeError:\n",
    "        content_book['Tematica']=None\n",
    "    #titulo articulo\n",
    "    try:\n",
    "        titu=s_item.find('h1', class_='article-details-fulltitle').get_text(strip=True)\n",
    "        content_book['Titulo Articulo']=titu\n",
    "    except AttributeError:\n",
    "        content_book['Titulo Articulo']=None\n",
    "    #resumen\n",
    "    try:\n",
    "        resu=s_item.find('div', class_='article-details-block article-details-abstract').get_text(strip=True)\n",
    "        content_book['Resumen']=resu.replace(\"Resumen\",\"\")\n",
    "    except AttributeError:\n",
    "        content_book['Resumen']=None\n",
    "    #abstract\n",
    "    try:\n",
    "        abst=s_item.find('div', id='item cover_image').get_text(strip=True)\n",
    "        content_book['Abstract']=abst\n",
    "    except AttributeError:\n",
    "        content_book['Abstract']=None\n",
    "    #Link incial\n",
    "    try:\n",
    "        linkart=url\n",
    "        content_book['Link Articulo']=linkart\n",
    "    except AttributeError:\n",
    "        content_book['Link Articulo']=None\n",
    "    #link articulo\n",
    "    try:\n",
    "        link=s_item.find('a', class_='btn btn-primary').get('href')\n",
    "        content_book['Link PDF']=urljoin(url_root, link)\n",
    "    except AttributeError:\n",
    "        content_book['Link PDF']=None\n",
    "    return content_book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33dba7-b533-4919-bb6a-b278568a699a",
   "metadata": {},
   "source": [
    "El list_scraper hace un scrapeo de la cada uno de los artículos, ya que en datos_book se acumuló la información obtenida de los métodos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156f534-21dd-45ad-977e-3bc509e8338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scraper=list_scraper[0:810]\n",
    "datos_book=[]\n",
    "for idx, i in enumerate(list_scraper):\n",
    "    print(f'estas escrapeando la pag {idx}')\n",
    "    datos_book.append(scraper_book(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49203150-2b7c-4a3e-8792-2019005f97ad",
   "metadata": {},
   "source": [
    "La variable **datos_book** es un listado en el que se recolecto la información requerida por lo cual se convierte en un **DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2843f-312a-4bc4-af2d-337504ccda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogo=pd.DataFrame(datos_book)\n",
    "df_catalogo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c21d80-8204-44dc-8eb3-9969bea57c9c",
   "metadata": {},
   "source": [
    "La información solicitada se pasa a un **CSV**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e294d-127b-4fb6-8072-791533b743e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogo.to_csv('Revista75.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70948f23-3903-46cd-a3a5-096179e99f8d",
   "metadata": {},
   "source": [
    "Final del Código"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
